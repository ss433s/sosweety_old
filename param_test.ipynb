{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CPU times: user 12.3 s, sys: 3.11 s, total: 15.4 s\nWall time: 15.2 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "from sParser import Word, Parse_result, stanford_simplify, KB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('short_data.csv',encoding='utf8')\n",
    "df2 = df.loc[:,('id','phrase','explanation')]\n",
    "df3= df2.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_stop = open('哈工大停用词表.txt',encoding='utf8')\n",
    "f_stop_text = f_stop.read( )\n",
    "f_stop.close( )\n",
    "f_stop_seg_list=f_stop_text.split('\\n')\n",
    "\n",
    "len(f_stop_seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def jiebaCleanText(text):\n",
    "    #cut = jieba.cut(text)\n",
    "    #cut_list=list(cut)\n",
    "    #\n",
    "    #text_word_list=[]\n",
    "    #for myword in cut_list:\n",
    "    #    #if not(myword.strip() in f_stop_seg_list) and len(myword.strip())>1:\n",
    "    #    if not(myword.strip() in f_stop_seg_list):    \n",
    "    #        text_word_list.append(myword)\n",
    "    #return text_word_list\n",
    "    key_word_list = ['形容','比喻','指','表示','常']\n",
    "    try:\n",
    "        psg_list=[[x.word,x.flag] for x in psg.cut(text)]\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "    psg_key_list =[]\n",
    "    tmp_index1=-1\n",
    "    for i in range(len(psg_list)):\n",
    "        if psg_list[i][0] in key_word_list:\n",
    "            tmp_index1=i\n",
    "        if psg_list[i][0]=='。' and tmp_index1 !=-1:\n",
    "            psg_key_list+=psg_list[tmp_index1+1:i]\n",
    "            tmp_index1=-1\n",
    "    if len(psg_key_list) ==0:\n",
    "        psg_key_list = psg_list\n",
    "\n",
    "    final_list=[]    \n",
    "    no_label=0\n",
    "    for x in psg_key_list:\n",
    "        if not('不' in x[0]) and no_label ==0 and (x[1] == 'd' or (x[0].strip() in f_stop_seg_list) or (len(x[0].strip()) ==1 and x[1] !='a')):\n",
    "            continue\n",
    "        elif x[0] == '不':\n",
    "            no_label = 1\n",
    "        elif no_label == 1:\n",
    "            final_list.append('不'+x[0])\n",
    "            no_label=0\n",
    "        else:\n",
    "            final_list.append(x[0])\n",
    "\n",
    "    return final_list\n",
    "\n",
    "\n",
    "df3['exp_word'] = df3.apply(lambda row:jiebaCleanText(row['explanation']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv('exp_word.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "import gensim\n",
    "\n",
    "docs = df3['exp_word'] \n",
    "dictionary = corpora.Dictionary(docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guoyu9\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf\\lib\\site-packages\\gensim\\models\\ldamodel.py:775: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n",
      "C:\\Users\\guoyu9\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf\\lib\\site-packages\\gensim\\models\\ldamodel.py:582: RuntimeWarning: overflow encountered in exp2\n",
      "  perwordbound, np.exp2(-perwordbound), len(chunk), corpus_words\n"
     ]
    }
   ],
   "source": [
    "for number_topics in [100,500,1000,1500]:\n",
    "    for pass_number in [5,10,15,20]:\n",
    "        \n",
    "        lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=number_topics,passes=pass_number)\n",
    "        \n",
    "        #根据阈值找出对应的标签\n",
    "        topic_cut_off=0.15\n",
    "        word_cut_off=0.1\n",
    "\n",
    "        result=[]\n",
    "        for i in range(len(df3)):\n",
    "            match={}\n",
    "            match['phrase']=df3['phrase'][i]\n",
    "            match['explanation']=docs[i]\n",
    "            i_lda = lda[dictionary.doc2bow(docs[i])]\n",
    "            for topic,p in i_lda:\n",
    "                if p>topic_cut_off:\n",
    "                    for word,p_word in lda.show_topic(topic,topn=10):\n",
    "                        if p_word >word_cut_off:\n",
    "                            match['label']=word\n",
    "                            match['p_topic']=p\n",
    "                            match['p_word']=p_word\n",
    "                            result.append(match)\n",
    "        df5=pd.DataFrame(result)\n",
    "        \n",
    "        file_name='result_'+str(number_topics) + '_' +str(pass_number) +'.csv'\n",
    "        df5.to_csv(file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}